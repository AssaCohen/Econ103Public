\documentclass[11pt]{article}
\usepackage[margin = 1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{color}
\usepackage{hyperref}
\begin{document}

\section*{Empirical Example from Gelman and Hill, Chapter 3}
We need the package ``ARM'' to use the \texttt{display} function.
<<message=FALSE>>=
#install.packages("arm")
library('arm')
@
Load the data from my website and attach it for convenience.
<<>>=
data.url <- "http://www.ditraglia.com/econ103/child_test_data.csv"
data <- read.csv(data.url)
attach(data)
@
Let's take a quick look at the dataset
<<>>=
head(data)
@
Here's a description of the data:
\begin{table}[h]
\centering
  \begin{tabular}{|ll|}
  \hline
		Variable Name & Description\\
		\hline
		\texttt{kid.score}& Child's Test Score at Age 3\\
		\texttt{mom.age}&Age of Mother at Birth of Child\\
		\texttt{mom.hs}& Mother Completed High School? (1 = Yes)\\
		\texttt{mom.iq}& Mother's IQ Score\\
    \hline
	\end{tabular}
\end{table}
\subsection*{First Regression: \texttt{mom.hs}}
This regression compares average test scores of children whose mother completed high school to those whose mother didn't.
<<>>=
reg1 <- lm(kid.score ~ mom.hs)
display(reg1)
@
Rounding, we can summarize the fitted model as follows:
  $$\mbox{\texttt{kid.score}} = \Sexpr{round(coef(reg1)[1],0)} + \Sexpr{round(coef(reg1)[2],0)} \cdot \mbox{\texttt{mom.hs}} + \mbox{error} $$
Remember that \texttt{mom.hs} is a binary variable, aka a dummy variable: it takes the value 1 if a child's mother completed high school and zero otherwise. Hence, this regression is the \emph{same thing} as comparing the mean test scores of two groups: those whose mother completed high school and those whose mother didn't! 
    \begin{eqnarray*}
      (\mbox{\texttt{mom.hs}} = 1) \Rightarrow \texttt{kid.score} &=&\Sexpr{round(coef(reg1)[1],0)} + \Sexpr{round(coef(reg1)[2],0)} \cdot 1 +\mbox{error}\\
      &=& \Sexpr{round(coef(reg1)[1],0) + round(coef(reg1)[2],0)}+\mbox{error} \\\\
    (\mbox{\texttt{mom.hs}} = 0) \Rightarrow\texttt{kid.score} &=& \Sexpr{round(coef(reg1)[1],0)} + \Sexpr{round(coef(reg1)[2],0)} \cdot 0 +\mbox{error}\\
    &=& \Sexpr{round(coef(reg1)[1],0) } + \mbox{error}
    \end{eqnarray*}
The difference of means simply equals the coefficient on \texttt{mom.hs}, namely $\Sexpr{round(coef(reg1)[2],0)}$.
Creating a confidence interval for this difference of means is easy, since R has already calculated the required standard error for us. Rounding, this value is approximately $\Sexpr{round(se.coef(reg1)[2], 1)}$, so our approximate 95\% confidence interval for the difference of means (the coefficient on \texttt{mom.hs}) is $\Sexpr{round(coef(reg1)[2],0)} \pm \Sexpr{2 * round(se.coef(reg1)[2], 1)}$, in other words $(\Sexpr{round(coef(reg1)[2],0) - 2 * round(se.coef(reg1)[2], 1)}, \Sexpr{round(coef(reg1)[2],0) + 2 * round(se.coef(reg1)[2], 1)})$. Since this interval doesn't include zero, we can reject the null that children whose mothers completed high school have the same test scores on average as those whose mothers didn't against the two-sided alternative at the 5\% significance level. It looks like children whose mothers completed high school do better on this test.

\subsection*{Second Regression: \texttt{mom.iq}}
Now a regression comparing average child test scores across mothers with different IQs. This is a continuous rather than binary predictor.
<<>>=
reg2 <- lm(kid.score ~ mom.iq)
display(reg2)
@
Rounding, we can summarize the fitted model as follows:
  $$\mbox{\texttt{kid.score}} = \Sexpr{round(coef(reg2)[1],0)} + \Sexpr{round(coef(reg2)[2],1)} \cdot \mbox{\texttt{mom.iq}} + \mbox{error} $$
The intercept in this model is not interpretable: it is the predicted test score for a child whose mother has an IQ of zero! The coefficient on \texttt{mom.iq} is meaningful, however. If we compare two groups of students who differ by one point in mothers' IQ, we would predict that the group with higher mother IQ will score $\Sexpr{round(coef(reg2)[2],1)}$ points higher, on average.

We can plot the data as follows
<<>>=
plot(mom.iq, kid.score, pch = 20, xlab = 'Mother IQ Score', ylab = 'Child Test Score')
@
To add the regression line, we need to extract the slope and intercept from the fitted regression:
<<>>=
coef(reg1)
intercept <- coef(reg2)[1]
slope <- coef(reg2)[2]
plot(mom.iq, kid.score, pch = 20, xlab = 'Mother IQ Score', ylab = 'Child Test Score')
abline(a = intercept, b = slope)
@


\subsection*{Third Regression: \texttt{mom.hs} and \texttt{mom.iq}}
Now we'll fit a regression with both \texttt{mom.hs} and \texttt{mom.iq}
<<>>=
reg3 <- lm(kid.score ~ mom.hs + mom.iq)
display(reg3)
@
Rounding, we can summarize the fitted model as follows:
  $$\mbox{\texttt{kid.score}} = \Sexpr{round(coef(reg3)[1],0)} + \Sexpr{round(coef(reg3)[2],1)} \cdot \mbox{\texttt{mom.hs}} +\Sexpr{round(coef(reg3)[3],1)} \cdot \mbox{\texttt{mom.iq}} + \mbox{error} $$
  Since \texttt{mom.hs} is binary, this is equivalent to letting each group have a regression line with a \emph{different intercept} but the same slope:
  \begin{eqnarray*}
    (\texttt{mom.hs = 1})\Rightarrow \texttt{kid.score} &=& \Sexpr{round(coef(reg3)[1],0)} + \Sexpr{round(coef(reg3)[2],0)} \cdot 1 + \Sexpr{round(coef(reg3)[3],1)}\cdot \texttt{mom.iq} + \mbox{error}\\
      &=& \Sexpr{round(coef(reg3)[1],0) + round(coef(reg3)[2],0)} + \Sexpr{round(coef(reg3)[3],1)}\cdot \texttt{mom.iq} + \mbox{error}\\ \\
    (\texttt{mom.hs = 0})\Rightarrow \texttt{kid.score} &=& \Sexpr{round(coef(reg3)[1],0)} + \Sexpr{round(coef(reg3)[2],0)} \cdot 0 + \Sexpr{round(coef(reg3)[3],1)}\cdot \texttt{mom.iq} + \mbox{error}\\
      &=&\Sexpr{round(coef(reg3)[1],0)} + \Sexpr{round(coef(reg3)[3],1)}\cdot \texttt{mom.iq} + \mbox{error}
  \end{eqnarray*}
In this case the intercept is not interpretable: it corresponds to the average test score for children whose mother did not complete high school and have a zero IQ! The other two coefficients, however, are meaningful. The coefficient on \texttt{mom.hs} compares test scores for children whose mothers have the same IQ but differ in whether or not they completed high school. The coefficient on \texttt{mom.iq} compares children whose mothers have the same value of \texttt{mom.iq} but differ in IQ by one point. 

We can plot the data and fitted models as follows. We'll plot the children whose mothers went to high school in \emph{gray} and those whose mothers didn't in \emph{black}.
<<>>=
colors <- ifelse (mom.hs == 0, "black", "gray")
plot(mom.iq, kid.score, pch = 20, xlab = 'Mother IQ Score', ylab = 'Child Test Score', col = colors)
@
To add the fitted regression lines we need to extract the common slope as well as the intercept for \emph{each group}
<<>>=
coef(reg3)
slope <- coef(reg3)[3]
intercept.hs <- coef(reg3)[1] + coef(reg3)[2]
intercept.no.hs <- coef(reg3)[1] 
@
Now we can make the plot, and add the regression lines with colors to match the points:
<<>>=
colors <- ifelse (mom.hs == 0, "black", "gray")
plot(mom.iq, kid.score, pch = 20, xlab = 'Mother IQ Score', ylab = 'Child Test Score', col = colors)
abline(a = intercept.hs, b = slope, col = 'gray')
abline(a = intercept.no.hs, b = slope, col = 'black')
@

\subsection*{Fourth Regression: \texttt{mom.hs}, \texttt{mom.iq}  and \texttt{mom.hs:mom.iq}}
Now we'll add an interaction between \texttt{mom.hs} and \texttt{mom.iq}: that is, we'll include a predictor whose value equals the \emph{product} of these two variables. The way to express this in R is \texttt{mom.hs:mom.iq}. Consider a child whose mother completed high school (\texttt{mom.hs}=1). For this child, $\texttt{mom.hs:mom.iq} = 1 \cdot \texttt{mom.iq} = \texttt{mom.iq}$. For a child whose mother did \emph{not} complete high school, $\texttt{mom.hs:mom.iq} = 0 \cdot \texttt{mom.iq} = 0$. At turns out that adding this interaction allows the two groups (those whose mother completed high school and those whose mother did not) to have regression lines with different slopes. First we'll fit the model:
<<>>=
reg4 <- lm(kid.score ~ mom.hs + mom.iq + mom.hs:mom.iq)
display(reg4)
@
Rounding, we can summarize the results as follows:
  \begin{eqnarray*}
    (\texttt{mom.hs = 1})\Rightarrow \texttt{kid.score} &=& \Sexpr{round(coef(reg4)[1],0)} + \Sexpr{round(coef(reg4)[2],0)} \cdot 1 + \Sexpr{round(coef(reg4)[3],1)}\cdot \texttt{mom.iq} \Sexpr{round(coef(reg4)[4],1)}\cdot 1 \cdot \texttt{mom.iq} +\mbox{error}\\
      &=& \Sexpr{round(coef(reg4)[1],0) + round(coef(reg4)[2],0)} + \Sexpr{round(coef(reg4)[3],1) + round(coef(reg4)[4],1)}\cdot \texttt{mom.iq} + \mbox{error}\\ \\
    (\texttt{mom.hs = 0})\Rightarrow \texttt{kid.score} &=& \Sexpr{round(coef(reg4)[1],0)} + \Sexpr{round(coef(reg4)[2],0)} \cdot 0 + \Sexpr{round(coef(reg4)[3],1)}\cdot \texttt{mom.iq} \Sexpr{round(coef(reg4)[4],1)}\cdot 0 \cdot \texttt{mom.iq} +\mbox{error}\\
      &=&\Sexpr{round(coef(reg4)[1],0)} + \Sexpr{round(coef(reg4)[3],1)}\cdot \texttt{mom.iq} + \mbox{error}
  \end{eqnarray*}



We can plot the two regression lines as follows. This time we need to allow different slopes \emph{as well as intercepts}.
<<>>=
coef(reg4)
slope.hs <- coef(reg4)[3] + coef(reg4)[4]
slope.no.hs <- coef(reg4)[3]
intercept.hs <- coef(reg4)[1] + coef(reg4)[2]
intercept.no.hs <- coef(reg4)[1] 
colors <- ifelse (mom.hs == 0, "black", "gray")
plot(mom.iq, kid.score, pch = 20, xlab = 'Mother IQ Score', ylab = 'Child Test Score at Age 3', col = colors)
abline(a = intercept.hs, b = slope.hs, col = 'gray')
abline(a = intercept.no.hs, b = slope.no.hs, col = 'black')
@

\end{document}